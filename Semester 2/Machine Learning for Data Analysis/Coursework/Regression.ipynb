{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS985/6 Spotify Regression Problem 2024\n",
    "\n",
    "## Group 14\n",
    "\n",
    "|Name |Student No.|\n",
    "|-----|-----------|\n",
    "|[Ishita Namdeo](ishita.namdeo.2023@uni.strath.ac.uk)| 202353325|\n",
    "|[Mohamed Tarek Mokhtar Omara Ahmed](mohamed.t.ahmed.2023@uni.strath.ac.uk)|202356621|\n",
    "|[Ramandeep Gil](ramandeep.gill.2023@uni.strath.ac.uk) ||\n",
    "|[Riley Simpson](riley.simpson.2019@uni.strath.ac.uk) | 202363053|\n",
    "|[S A Nawash Akhtar](nawash.akhtar.2023@uni.strath.ac.uk) |202352528|\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors Selection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the proper modules \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessery libraries & pre-processing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures , StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Import all relevant regressors from sklearn  \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import BayesianRidge,ElasticNet,LogisticRegression,SGDRegressor,LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import LinearSVR,SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset \n",
    "In order to train the regressors , we will use the cleaned/processed training datasets created in the [Preprocessing.ipynb](Preprocessing.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Training_Cleaned.csv\")\n",
    "df.head()\n",
    "x = df.drop(columns=['pop','Id'])\n",
    "y = df[\"pop\"]\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Random state\n",
    ">\"Using random_state is important for reproducibility, debugging, and comparison of results. By setting this parameter, you can >ensure that your experiments are reproducible, debug problems more effectively, and compare the performance of different models >more accurately\" -[What Is 'random_state' in sklearn.model_selection.train_test_split Example?](https://saturncloud.io/blog/what-is-randomstate-in-sklearnmodelselectiontraintestsplit-example/#:~:text=Using%20random_state%20is%20important%20for%20reproducibility%2C%20debugging%2C%20and%20comparison%20of,of%20different%20models%20more%20accurately.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enabling the regressors \n",
    "From here we can adjust the hyperparameters adjust the performance of each one.\n",
    "\n",
    "#### GradientBoostingRegressor (GBR)\n",
    "- Gradient Boosting is an ensemble learning technique that builds models sequentially, with each model trying to correct the    errors of its predecessor. The default settings are often a good starting point.\n",
    "KernelRidge (KR):\n",
    "\n",
    "#### Kernel Ridge Regression\n",
    "- Combines Ridge Regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear data, this can be very effective.\n",
    "BayesianRidge (BR):\n",
    "\n",
    "#### Bayesian Ridge Regression \n",
    "- implements Bayesian linear regression. It is particularly useful when the size of the data is not too large and you want to include regularization parameters that are tuned to the data.\n",
    "\n",
    "#### ElasticNet (EN)\n",
    "- ElasticNet is a linear regression model trained with both l1 and l2-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "\n",
    "#### DecisionTreeRegressor (DT)\n",
    "- 'friedman_mse' criterion measures the quality of a split based on mean squared error with improvement score by Friedman. It's often a good choice for regression.\n",
    "- 'max_depth=20' sets a limit on the depth of the tree. Deep trees can lead to overfitting.\n",
    "\n",
    "Other parameters like 'max_features', 'max_leaf_nodes', and 'min_impurity_decrease' are set to their default values, which are generally a good starting point.\n",
    "- 'min_samples_leaf' and 'min_samples_split' control the size of the leaf nodes and splits, influencing model complexity.\n",
    "- 'random_state' ensures reproducibility.\n",
    "- 'splitter=best' chooses the best split at each node.\n",
    "\n",
    "#### LinearSVR\n",
    "- 'epsilon=8' defines the margin of tolerance where no penalty is given to errors. The choice of this value can significantly affect the fit.\n",
    "- 'random_state' for reproducibility.\n",
    "\n",
    "#### SVR with Polynomial Kernel (Poly_SVR)\n",
    "- 'kernel=\"poly\"' specifies the use of a polynomial kernel.\n",
    "- 'degree=2' for the polynomial kernel. A degree of 2 usually captures non-linear relationships well without overfitting.\n",
    "- 'C=1' sets the regularization parameter. A smaller value of C means more regularization.\n",
    "- 'epsilon=0.1' sets the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "- 'gamma=\"scale\"' automatically adjusts gamma based on the number of features, which often leads to better performance.\n",
    "\n",
    "#### LogisticRegression\n",
    "Used for classification problems. The default parameters are often a reasonable starting point.\n",
    "- 'random_state' for reproducibility.\n",
    "\n",
    "#### SGDRegressor\n",
    "Stochastic Gradient Descent is a simple yet very efficient approach to fitting linear models.\n",
    "- 'random_state' for reproducibility.\n",
    "\n",
    "### LinearRegression\n",
    "Standard linear regression without regularization. It's the most basic form of regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR = GradientBoostingRegressor()\n",
    "KR = KernelRidge()\n",
    "BR = BayesianRidge()\n",
    "EN = ElasticNet()\n",
    "DT = DecisionTreeRegressor()\n",
    "Lin_SVR = LinearSVR(epsilon = 8 , random_state=random_state )\n",
    "Poly_SVR = SVR(kernel=\"poly\", degree=2, C=1, epsilon=0.1, gamma=\"scale\")\n",
    "Log_reg = LogisticRegression(random_state=random_state)\n",
    "SGD = SGDRegressor(random_state=random_state)\n",
    "Lin_reg = LinearRegression()\n",
    "\n",
    "# Combining these models into a list allows for a simple training/testing function to be used for each of them. \n",
    "reg_list = [GBR,KR,BR,DT, Lin_SVR, Lin_reg, Log_reg, SGD,Poly_SVR]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Data \n",
    "Scaling our dataset before training / testing presents a few advantages:\n",
    "\n",
    "- **Feature Standardization**: Our dataset contains numerical features like `bpm`, `nrgy`, `dnce`, etc., with varying scales. StandardScaler() will standardize these features to have a mean of 0 and a standard deviation of 1, ensuring that no single feature will dominate others during the learning process.\n",
    "\n",
    "- **Model Performance**: Some of the regressor models are sensitive to scale (such as linear and non-linear SVR) , standardizing the features will likely improve model performance.\n",
    "\n",
    "- **Gradient Descent Optimization**: For models that use gradient descent (like linear regression or neural networks), having features on the same scale can speed up convergence.\n",
    "\n",
    "- **Handling Skewed Distributions**: If any of your numerical features are not normally distributed, `StandardScaler()` can mitigate the effect of skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "x_train = std_scaler.fit_transform(x_train)\n",
    "x_test = std_scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Regressor Function \n",
    "In order to determine which regressor is most suitable for the task , we need to compare different regression models and evaluate their mean squared error. To do this the following steps must be taken:\n",
    "\n",
    "1. Train the regressor on the training dataset \n",
    "2. Predict the popularity of the songs in the testing dataset\n",
    "3. Compare these predicted values with the real values via the mean squared error: $$RMSD = \\sqrt{\\frac{\\sum\\limits_{i=1}^{N} (x_{i} - \\hat{x_{i}})}{N}}$$\n",
    "\n",
    "In order to streamline the selection process these steps were summarised in the function `use_regressor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:11.470727253839497 for GradientBoostingRegressor\n",
      "MSE:63.54870272274179 for KernelRidge\n",
      "MSE:10.746770268144891 for BayesianRidge\n",
      "MSE:14.941644730508996 for DecisionTreeRegressor\n",
      "MSE:12.22028355474507 for LinearSVR\n",
      "MSE:572248032970038.9 for LinearRegression\n",
      "MSE:11.797746106219323 for LogisticRegression\n",
      "MSE:10.97351120861915 for SGDRegressor\n",
      "MSE:12.112180434207856 for SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riley\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def use_regressor(reg):\n",
    "    regressor = reg.fit(x_train,y_train)\n",
    "    y_pred = regressor.predict(x_test)\n",
    "    mse=mean_squared_error(y_test,y_pred)\n",
    "    print(f\"MSE:{np.sqrt(mse)} for {reg.__class__.__name__}\")\n",
    "    return y_pred\n",
    "\n",
    "for reg in reg_list:\n",
    "    use_regressor(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "\n",
    "| Model                  | MSE                  |\n",
    "|------------------------|----------------------|\n",
    "| **GradientBoostingRegressor** | <mark>9.478579817350175</mark>   |\n",
    "| KernelRidge            | 61.455887080158426  |\n",
    "| BayesianRidge          | 9.561217353361643   |\n",
    "| DecisionTreeRegressor  | 14.408642349609984  |\n",
    "| LinearSVR              | 10.509947931975468  |\n",
    "| LinearRegression       | 335209587401381.75  |\n",
    "| LogisticRegression     | 13.643015360684315  |\n",
    "| SGDRegressor           | 9.864147214378303   |\n",
    "| SVR                    | 12.962918698414367  |\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique that combines multiple weak prediction models (typically decision trees) to create a strong predictive model. By sequentially correcting the mistakes of previous models. \n",
    "\n",
    "The training dataset consisted of 453 rows and 122 columns, featuring a diverse range of features. Key features include 'bpm' (beats per minute), 'nrgy' (energy), 'dnce' (danceability), 'dB' (loudness), 'live' (liveness), 'val' (valence), 'dur' (duration), 'acous' (acousticness), 'spch' (speechiness), and 'pop' (popularity), along with several decade-related binary features (indicating the song's decade) and various statistics about the song title and artist name (like length and word count). \n",
    "\n",
    "These wide range of values in the features indicate a rich and varied collection of songs with different characteristics. This diversity, combined with the presence of both numerical and categorical data, likely contributed to the effective performance of the GradientBoostingRegressor, which excels in handling complex and non-linear relationships, is robust against overfitting, and can leverage the benefits of ensemble learning for predictive accuracy.\n",
    "\n",
    "> As such Gradient Boosting Regressor was chosen for our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Regressor\n",
    "Before applying a trained regressor to the new testing dataset , the model must be optimised first based on the data it's being tested on. [Grid Search CV](https://www.mygreatlearning.com/blog/gridsearchcv/) will be used for this. \n",
    "\n",
    "Parameters like `loss`, `learning_rate`, `n_estimators`, m`ax_depth, `min_samples_split`, and `min_samples_leaf` are chosen due to their significant impact on the model's learning dynamics and generalization capabilities. \n",
    "\n",
    "The loss function, including 'squared_error' and 'huber', is crucial as it dictates how the model penalizes errors, with 'huber' being particularly useful for handling outliers. \n",
    "\n",
    "The learning_rate influences the speed and robustness of the learning process, and a range of values is selected to explore the trade-off between fast learning and the risk of overfitting. \n",
    "\n",
    "The n_estimators parameter, determining the number of boosting stages, is pivotal in defining the model complexity and is set to moderate values to balance accuracy and overfitting.\n",
    "\n",
    "The choice of max_depth, `min_samples_split`, and `min_samples_leaf` is aimed at controlling the model structure to prevent overfitting while capturing sufficient data patterns. The max_depth of the trees strikes a balance between learning detailed data relationships and maintaining model simplicity. \n",
    "\n",
    "Similarly, `min_samples_split` and `min_samples_leaf` ensure that splits and leaf nodes do not cater to overly specific or minute data samples, thereby smoothing the model and enhancing its ability to generalize. These parameters are set to a range that allows exploration of different model behaviors while keeping the computational load manageable, ensuring an efficient yet thorough search for the optimal model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best parameters found:  {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best MSE:  102.54963136970562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(GradientBoostingRegressor(random_state=random_state), \n",
    "                           param_grid, \n",
    "                           scoring='neg_mean_squared_error', \n",
    "                           cv=5, \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best MSE: \",np.sqrt( -grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Popularity with training dataset\n",
    "This next step is similar to previous ones with the addition of a predictions dataframe which we use to store our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_url =  \"https://raw.githubusercontent.com/Riley-Simpson/Postgraduate-Work-/main/Semester%202/Machine%20Learning%20for%20Data%20Analysis/Coursework/Training_cleaned.csv\"\n",
    "testing_url = \"https://raw.githubusercontent.com/Riley-Simpson/Postgraduate-Work-/main/Semester%202/Machine%20Learning%20for%20Data%20Analysis/Coursework/Testing_Cleaned.csv\"\n",
    "df = pd.read_csv(training_url)\n",
    "df.head()\n",
    "x_train = df.drop(columns=['pop'])\n",
    "y_train = df[\"pop\"]\n",
    "x_test = pd.read_csv(testing_url)\n",
    "predictions = pd.DataFrame(x_test['Id']) \n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "x_train = std_scaler.fit_transform(x_train)\n",
    "x_test = std_scaler.fit_transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import Gradient Boosting Regressor from sklearn.\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Set a fixed random state for reproducibility.\n",
    "random_state = 42\n",
    "\n",
    "# Initialize Gradient Boosting Regressor with specified settings.\n",
    "GBR = GradientBoostingRegressor(learning_rate=0.1, max_depth=3, min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "\n",
    "# Train the model on the training data.\n",
    "regressor = GBR.fit(x_train, y_train)\n",
    "\n",
    "# Predict values using the model on the test data.\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "# Convert predictions to a list.\n",
    "predictions_list = list(y_pred)\n",
    "\n",
    "# Add predictions to a DataFrame.\n",
    "predictions['pop'] = predictions_list\n",
    "\n",
    "# Save the predictions to a CSV file.\n",
    "predictions.to_csv(f'{GBR.__class__.__name__}_pop_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition Results \n",
    "\n",
    "As of 18/02/2024 , 15:50 : CS985 Group 14 are `7th` with a score of `7.14804`\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
